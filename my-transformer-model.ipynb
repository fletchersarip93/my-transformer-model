{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        positions = torch.arange(max_seq_len).unsqueeze(dim=1)\n",
    "        feat_indexes = torch.arange(start=0, end=feat_dim, step=2)\n",
    "        \n",
    "        self.positional_encoding = torch.zeros(max_seq_len, feat_dim)\n",
    "        \n",
    "        positions_feat_idx_matrix = positions / (10000 ** (feat_indexes / feat_dim))\n",
    "        \n",
    "        self.positional_encoding[:, 0::2] = torch.sin(positions_feat_idx_matrix)\n",
    "        self.positional_encoding[:, 1::2] = torch.cos(positions_feat_idx_matrix)\n",
    "    \n",
    "    # x shape is (batch, sequence, elements)\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:x.shape[-2], :x.shape[-1]].unsqueeze(dim=0)\n",
    "    \n",
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, input_dimension, key_dimension, value_dimension):\n",
    "        super().__init__()\n",
    "        self.query_projection = torch.nn.Linear(in_features=input_dimension, out_features=key_dimension)\n",
    "        self.key_projection = torch.nn.Linear(in_features=input_dimension, out_features=key_dimension)\n",
    "        self.value_projection = torch.nn.Linear(in_features=input_dimension, out_features=value_dimension)\n",
    "        self.attention_scale = math.sqrt(key_dimension)\n",
    "    \n",
    "    # x dimension is (batch, sequence, embedding)\n",
    "    def forward(self, query, key, value):\n",
    "        projected_query = self.query_projection(query) # (batch, sequence, key_dimension)\n",
    "        projected_key = self.key_projection(key) # (batch, sequence, key_dimension)\n",
    "        projected_value = self.value_projection(value) # (batch, sequence, value_dimension)\n",
    "        scaled_dot_product = torch.matmul(projected_query, torch.transpose(projected_key, 1, 2)) / self.attention_scale # (batch, sequence, sequence)\n",
    "        attention = torch.nn.functional.softmax(scaled_dot_product, dim=-1) # (batch, sequence, sequence)\n",
    "        \n",
    "        return torch.matmul(attention, projected_value) # (batch, sequence, value_dimension)\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_head, input_dimension, key_dimension, value_dimension, output_dimension):\n",
    "        super().__init__()\n",
    "        self.attention_heads = [AttentionHead(input_dimension=input_dimension, key_dimension=key_dimension, value_dimension=value_dimension) for _ in range(num_head)]\n",
    "        self.linear_output = torch.nn.Linear(in_features=num_head * value_dimension, out_features=output_dimension)\n",
    "    \n",
    "    # x dimension is (batch, sequence, embedding)\n",
    "    def forward(self, query, key, value):\n",
    "        head_results = [attention_head(query=query, key=key, value=value) for attention_head in self.attention_heads] # (batch, sequence, value_dimension)\n",
    "        concatenated_heads = torch.cat(head_results, dim=-1) # (batch, sequence, value_dimension * num_heads)\n",
    "        \n",
    "        return self.linear_output(concatenated_heads) # (batch, sequence, output_dimension)\n",
    "    \n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, num_head, input_dimension, key_dimension, value_dimension, ff_inner_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            num_head=num_head,\n",
    "            input_dimension=input_dimension,\n",
    "            key_dimension=key_dimension,\n",
    "            value_dimension=value_dimension,\n",
    "            output_dimension=input_dimension,\n",
    "        )\n",
    "                \n",
    "        self.layer_norm_mha = torch.nn.LayerNorm(normalized_shape=input_dimension)\n",
    "        \n",
    "        # force output dimension = input dimension so that we can do residual connection\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=input_dimension, out_features=ff_inner_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=ff_inner_dim, out_features=input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm_ff = torch.nn.LayerNorm(normalized_shape=input_dimension)\n",
    "    \n",
    "    # x dimension is (batch, sequence, embedding)\n",
    "    def forward(self, x):\n",
    "        mha = self.multi_head_attention(query=x, key=x, value=x) # (batch, sequence, input_dimension)\n",
    "        x = self.layer_norm_mha(x + mha) # (batch, sequence, input_dimension)\n",
    "        feed_forward = self.feed_forward(x) # (batch, sequence, input_dimension)\n",
    "        \n",
    "        return self.layer_norm_ff(x + feed_forward) # (batch, sequence, input_dimension)\n",
    "    \n",
    "class TransformerDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_head, input_dimension, key_dimension, value_dimension, ff_inner_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            num_head=num_head,\n",
    "            input_dimension=input_dimension,\n",
    "            key_dimension=key_dimension,\n",
    "            value_dimension=value_dimension,\n",
    "            output_dimension=input_dimension,\n",
    "        )\n",
    "                \n",
    "        self.layer_norm_mha = torch.nn.LayerNorm(normalized_shape=input_dimension)\n",
    "        \n",
    "        self.encoder_multi_head_attention = MultiHeadAttention(\n",
    "            num_head=num_head,\n",
    "            input_dimension=input_dimension,\n",
    "            key_dimension=key_dimension,\n",
    "            value_dimension=value_dimension,\n",
    "            output_dimension=input_dimension,\n",
    "        )\n",
    "                \n",
    "        self.layer_norm_encoder_mha = torch.nn.LayerNorm(normalized_shape=input_dimension)\n",
    "        \n",
    "        # force output dimension = input dimension so that we can do residual connection\n",
    "        self.feed_forward = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=input_dimension, out_features=ff_inner_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=ff_inner_dim, out_features=input_dimension)\n",
    "        )\n",
    "        \n",
    "        self.layer_norm_ff = torch.nn.LayerNorm(normalized_shape=input_dimension)\n",
    "    \n",
    "    # x dimension is (batch, sequence, embedding)\n",
    "    def forward(self, x, encoder_output):\n",
    "        mha = self.multi_head_attention(query=x, key=x, value=x) # (batch, sequence, input_dimension)\n",
    "        x = self.layer_norm_mha(x + mha) # (batch, sequence, input_dimension)\n",
    "        \n",
    "        encoder_mha = self.encoder_multi_head_attention(query=x, key=encoder_output, value=encoder_output) # (batch, sequence, input_dimension)\n",
    "        x = self.layer_norm_encoder_mha(x + encoder_mha) # (batch, sequence, input_dimension)\n",
    "        \n",
    "        feed_forward = self.feed_forward(x) # (batch, sequence, input_dimension)\n",
    "        \n",
    "        return self.layer_norm_ff(x + feed_forward) # (batch, sequence, input_dimension)\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dimension,\n",
    "                 encoder_layer_num,\n",
    "                 encoder_head_num,\n",
    "                 encoder_key_dimension,\n",
    "                 encoder_value_dimension,\n",
    "                 encoder_ff_inner_dim,\n",
    "                 decoder_layer_num,\n",
    "                 decoder_head_num,\n",
    "                 decoder_key_dimension,\n",
    "                 decoder_value_dimension,\n",
    "                 decoder_ff_inner_dim,\n",
    "                 output_dimension\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(feat_dim=input_dimension)\n",
    "        \n",
    "        self.encoders = torch.nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoder(\n",
    "                    num_head=encoder_head_num,\n",
    "                    input_dimension=input_dimension,\n",
    "                    key_dimension=encoder_key_dimension,\n",
    "                    value_dimension=encoder_value_dimension,\n",
    "                    ff_inner_dim=encoder_ff_inner_dim\n",
    "                )\n",
    "                for _ in range(encoder_layer_num)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.decoders = [\n",
    "            TransformerDecoder(\n",
    "                num_head=decoder_head_num,\n",
    "                input_dimension=input_dimension,\n",
    "                key_dimension=decoder_key_dimension,\n",
    "                value_dimension=decoder_value_dimension,\n",
    "                ff_inner_dim=decoder_ff_inner_dim\n",
    "            )\n",
    "            for _ in range(decoder_layer_num)\n",
    "        ]\n",
    "        \n",
    "        self.output_linear = torch.nn.Linear(in_features=input_dimension, out_features=output_dimension)\n",
    "        \n",
    "    def forward(self, input_embedding, output_embedding):\n",
    "        input_embedding = self.positional_encoding(input_embedding)\n",
    "        \n",
    "        encoder_output = self.encoders(input_embedding)\n",
    "        \n",
    "        decoder_output = self.positional_encoding(output_embedding)\n",
    "        \n",
    "        for decoder in self.decoders:\n",
    "            decoder_output = decoder(decoder_output, encoder_output)\n",
    "        \n",
    "        return self.output_linear(decoder_output) # (batch, sequence, output_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape is (sequence, embedding_dimension)\n",
    "EMBEDDING_DIMENSION = 512\n",
    "SEQ_LEN = 3\n",
    "\n",
    "input_embeddings_sequence = torch.randn((3, SEQ_LEN, EMBEDDING_DIMENSION))\n",
    "input_embeddings_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    input_dimension=input_embeddings_sequence.shape[-1],\n",
    "    encoder_layer_num=6,\n",
    "    encoder_head_num=8,\n",
    "    encoder_key_dimension=128,\n",
    "    encoder_value_dimension=128,\n",
    "    encoder_ff_inner_dim=2048,\n",
    "    decoder_layer_num=6,\n",
    "    decoder_head_num=8,\n",
    "    decoder_key_dimension=256,\n",
    "    decoder_value_dimension=256,\n",
    "    decoder_ff_inner_dim=2048,\n",
    "    output_dimension=3\n",
    ")\n",
    "\n",
    "result = model(input_embeddings_sequence, torch.randn((3, 5, EMBEDDING_DIMENSION)))\n",
    "\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-2.0.0-cpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
