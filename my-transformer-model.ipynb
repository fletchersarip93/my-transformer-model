{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport math","metadata":{"execution":{"iopub.status.busy":"2023-08-09T21:18:31.405837Z","iopub.execute_input":"2023-08-09T21:18:31.406185Z","iopub.status.idle":"2023-08-09T21:18:31.411231Z","shell.execute_reply.started":"2023-08-09T21:18:31.406154Z","shell.execute_reply":"2023-08-09T21:18:31.409706Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(torch.nn.Module):\n    def __init__(self, feat_dim, max_seq_len=5000):\n        super().__init__()\n        \n        positions = torch.arange(max_seq_len).unsqueeze(dim=1)\n        feat_indexes = torch.arange(start=0, end=feat_dim, step=2)\n        \n        self.positional_encoding = torch.zeros(max_seq_len, feat_dim)\n        \n        positions_feat_idx_matrix = positions / (10000 ** (feat_indexes / feat_dim))\n        \n        self.positional_encoding[:, 0::2] = torch.sin(positions_feat_idx_matrix)\n        self.positional_encoding[:, 1::2] = torch.cos(positions_feat_idx_matrix)\n    \n    # x shape is (batch, sequence, elements)\n    def forward(self, x):\n        return x + self.positional_encoding[:x.shape[-2], :x.shape[-1]].unsqueeze(dim=0)\n    \nclass AttentionHead(torch.nn.Module):\n    def __init__(self, input_dimension, key_dimension, value_dimension):\n        super().__init__()\n        self.query_projection = torch.nn.Linear(in_features=input_dimension, out_features=key_dimension)\n        self.key_projection = torch.nn.Linear(in_features=input_dimension, out_features=key_dimension)\n        self.value_projection = torch.nn.Linear(in_features=input_dimension, out_features=value_dimension)\n        self.attention_scale = math.sqrt(key_dimension)\n    \n    # x dimension is (batch, sequence, embedding)\n    def forward(self, query, key, value):\n        projected_query = self.query_projection(query) # (batch, sequence, key_dimension)\n        projected_key = self.key_projection(key) # (batch, sequence, key_dimension)\n        projected_value = self.value_projection(value) # (batch, sequence, value_dimension)\n        scaled_dot_product = torch.matmul(projected_query, torch.transpose(projected_key, 1, 2)) / self.attention_scale # (batch, sequence, sequence)\n        attention = torch.nn.functional.softmax(scaled_dot_product, dim=-1) # (batch, sequence, sequence)\n        \n        return torch.matmul(attention, projected_value) # (batch, sequence, value_dimension)\n\nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, num_head, input_dimension, key_dimension, value_dimension, output_dimension):\n        super().__init__()\n        self.attention_heads = [AttentionHead(input_dimension=input_dimension, key_dimension=key_dimension, value_dimension=value_dimension) for _ in range(num_head)]\n        self.linear_output = torch.nn.Linear(in_features=num_head * value_dimension, out_features=output_dimension)\n    \n    # x dimension is (batch, sequence, embedding)\n    def forward(self, query, key, value):\n        head_results = [attention_head(query=query, key=key, value=value) for attention_head in self.attention_heads] # (batch, sequence, value_dimension)\n        concatenated_heads = torch.cat(head_results, dim=-1) # (batch, sequence, value_dimension * num_heads)\n        \n        return self.linear_output(concatenated_heads) # (batch, sequence, output_dimension)\n    \nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, num_head, input_dimension, ff_inner_dim):\n        super().__init__()\n        \n        if input_dimension % num_head != 0:\n            raise Exception(\"input_dimension is not divisible by num_head!\")\n        \n        self.multi_head_attention = MultiHeadAttention(\n            num_head=num_head,\n            input_dimension=input_dimension,\n            key_dimension=input_dimension // num_head,\n            value_dimension=input_dimension // num_head,\n            output_dimension=input_dimension, # force output dimension = input dimension so that we can do residual connection\n        )\n        \n        self.layer_norm_mhsa = torch.nn.LayerNorm(normalized_shape=input_dimension)\n        \n        # force output dimension = input dimension so that we can do residual connection\n        self.feed_forward = torch.nn.Sequential(\n            torch.nn.Linear(in_features=input_dimension, out_features=ff_inner_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=ff_inner_dim, out_features=input_dimension)\n        )\n        \n        self.layer_norm_ff = torch.nn.LayerNorm(normalized_shape=input_dimension)\n    \n    # x dimension is (batch, sequence, embedding)\n    def forward(self, x):\n        mhsa = self.multi_head_attention(query=x, key=x, value=x) # (batch, sequence, input_dimension)\n        x = self.layer_norm_mhsa(x + mhsa) # (batch, sequence, input_dimension)\n        feed_forward = self.feed_forward(x) # (batch, sequence, input_dimension)\n        \n        return self.layer_norm_ff(x + feed_forward) # (batch, sequence, input_dimension)\n    \nclass TransformerDecoder(torch.nn.Module):\n    def __init__(self, num_head, input_dimension, ff_inner_dim):\n        super().__init__()\n        \n        if input_dimension % num_head != 0:\n            raise Exception(\"input_dimension is not divisible by num_head!\")\n        \n        self.multi_head_attention = MultiHeadAttention(\n            num_head=num_head,\n            input_dimension=input_dimension,\n            key_dimension=input_dimension // num_head,\n            value_dimension=input_dimension // num_head,\n            output_dimension=input_dimension, # force output dimension = input dimension so that we can do residual connection\n        )\n        \n        self.layer_norm_mhsa = torch.nn.LayerNorm(normalized_shape=input_dimension)\n        \n        self.encoder_multi_head_attention = MultiHeadAttention(\n            num_head=num_head,\n            input_dimension=input_dimension,\n            key_dimension=input_dimension // num_head,\n            value_dimension=input_dimension // num_head,\n            output_dimension=input_dimension, # force output dimension = input dimension so that we can do residual connection\n        )\n        \n        self.layer_norm_encoder_mhsa = torch.nn.LayerNorm(normalized_shape=input_dimension)\n        \n        # force output dimension = input dimension so that we can do residual connection\n        self.feed_forward = torch.nn.Sequential(\n            torch.nn.Linear(in_features=input_dimension, out_features=ff_inner_dim),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=ff_inner_dim, out_features=input_dimension)\n        )\n        \n        self.layer_norm_ff = torch.nn.LayerNorm(normalized_shape=input_dimension)\n    \n    # x dimension is (batch, sequence, embedding)\n    def forward(self, x, encoder_output):\n        mhsa = self.multi_head_attention(query=x, key=x, value=x) # (batch, sequence, input_dimension)\n        x = self.layer_norm_mhsa(x + mhsa) # (batch, sequence, input_dimension)\n        \n        encoder_mhsa = self.encoder_multi_head_attention(query=x, key=encoder_output, value=encoder_output) # (batch, sequence, input_dimension)\n        x = self.layer_norm_encoder_mhsa(x + encoder_mhsa) # (batch, sequence, input_dimension)\n        \n        feed_forward = self.feed_forward(x) # (batch, sequence, input_dimension)\n        \n        return self.layer_norm_ff(x + feed_forward) # (batch, sequence, input_dimension)\n\nclass Transformer(torch.nn.Module):\n    def __init__(self,\n                 input_dimension,\n                 encoder_layer_num,\n                 encoder_head_num,\n                 encoder_ff_inner_dim,\n                 decoder_layer_num,\n                 decoder_head_num,\n                 decoder_ff_inner_dim,\n                 output_dimension\n                ):\n        super().__init__()\n        \n        self.positional_encoding = PositionalEncoding(feat_dim=input_dimension)\n        \n        self.encoders = torch.nn.Sequential(\n            *[TransformerEncoder(num_head=encoder_head_num, input_dimension=input_dimension, ff_inner_dim=encoder_ff_inner_dim) for _ in range(encoder_layer_num)]\n        )\n        self.decoders = [TransformerDecoder(num_head=decoder_head_num, input_dimension=input_dimension, ff_inner_dim=decoder_ff_inner_dim) for _ in range(decoder_layer_num)]\n        \n        self.output_linear = torch.nn.Linear(in_features=input_dimension, out_features=output_dimension)\n        \n    def forward(self, input_embedding, output_embedding):\n        input_embedding = self.positional_encoding(input_embedding)\n        encoder_output = self.encoders(input_embedding)\n        \n        decoder_output = self.positional_encoding(output_embedding)\n        \n        for decoder in self.decoders:\n            decoder_output = decoder(decoder_output, encoder_output)\n        \n        return self.output_linear(decoder_output) # (batch, sequence, output_dimension)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T21:18:31.413121Z","iopub.execute_input":"2023-08-09T21:18:31.413531Z","iopub.status.idle":"2023-08-09T21:18:31.445634Z","shell.execute_reply.started":"2023-08-09T21:18:31.413505Z","shell.execute_reply":"2023-08-09T21:18:31.444577Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"# shape is (sequence, embedding_dimension)\nEMBEDDING_DIMENSION = 512\nSEQ_LEN = 3\n\ninput_embeddings_sequence = torch.randn((3, SEQ_LEN, EMBEDDING_DIMENSION))\ninput_embeddings_sequence.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-09T21:18:31.447446Z","iopub.execute_input":"2023-08-09T21:18:31.447740Z","iopub.status.idle":"2023-08-09T21:18:31.467224Z","shell.execute_reply.started":"2023-08-09T21:18:31.447715Z","shell.execute_reply":"2023-08-09T21:18:31.465767Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 3, 512])"},"metadata":{}}]},{"cell_type":"code","source":"model = Transformer(\n    input_dimension=input_embeddings_sequence.shape[-1],\n    encoder_layer_num=6,\n    encoder_head_num=8,\n    encoder_ff_inner_dim=2048,\n    decoder_layer_num=6,\n    decoder_head_num=8,\n    decoder_ff_inner_dim=2048,\n    output_dimension=3\n)\n\nresult = model(input_embeddings_sequence, torch.randn((3, 5, EMBEDDING_DIMENSION)))\n\nprint(result.shape)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T21:18:31.468339Z","iopub.execute_input":"2023-08-09T21:18:31.468867Z","iopub.status.idle":"2023-08-09T21:18:31.955478Z","shell.execute_reply.started":"2023-08-09T21:18:31.468838Z","shell.execute_reply":"2023-08-09T21:18:31.954197Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"torch.Size([3, 5, 3])\ntensor([[[ 0.1533, -0.2047, -0.0925],\n         [ 0.5067, -0.6973, -0.4356],\n         [ 0.1026, -0.2700, -0.4674],\n         [-0.1781, -0.1886,  0.1556],\n         [-0.0872,  0.2352,  0.1347]],\n\n        [[ 0.9160, -0.3647, -0.4792],\n         [ 0.7410, -0.4025, -0.2174],\n         [ 0.4471,  0.0610, -0.0307],\n         [ 1.0257, -0.6982, -0.5125],\n         [ 0.5325, -0.2835, -0.1735]],\n\n        [[-0.6674,  0.0289,  0.5842],\n         [-0.0039, -0.8921,  0.2027],\n         [ 0.0155, -0.2702,  0.9253],\n         [-0.3243, -0.2298,  0.8721],\n         [-0.5108, -0.6590,  0.1511]]], grad_fn=<ViewBackward0>)\n","output_type":"stream"}]}]}